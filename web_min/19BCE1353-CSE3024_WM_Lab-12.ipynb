{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463d05ae",
   "metadata": {},
   "source": [
    "## 19BCE1353 - Sahil Sachin Donde, CSE3024 WM Lab-12 - LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e6b61",
   "metadata": {},
   "source": [
    "### Take a collection of documents. Implement automatic topic extraction using LDA. \n",
    "### Draw your inferences.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37ecca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc5235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a483a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ada12",
   "metadata": {},
   "source": [
    "### Creating the  Corpus from the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a304aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "015779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "copus = []\n",
    "for i in range(5):\n",
    "    D = []\n",
    "    fn = \"Doc\"+str(i+1)+\".txt\"\n",
    "    #print(fn)\n",
    "    f = open(fn,'r')\n",
    "    cnt = 1\n",
    "    for x in f:\n",
    "        D.append(x)\n",
    "    corpus.append(D)\n",
    "corpus = (corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edd2b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3daacc",
   "metadata": {},
   "source": [
    "### Text Preprocessing on the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fd3aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop loss words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c7727",
   "metadata": {},
   "source": [
    "### After performing stop word removal, exluding punctuations and lemmatizationm following is cleaned corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5549818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book',\n",
       "  'helped',\n",
       "  'learn',\n",
       "  'much']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb1418",
   "metadata": {},
   "source": [
    "### Creating the term dictionary of our courpus that is of all the words (Sepcific to Genism syntax perspective), \n",
    "### where every unique term is assigned an index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f65b3bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(47 unique tokens: ['movie', 'want', 'watch', 'weekend', 'beating']...)\n"
     ]
    }
   ],
   "source": [
    "dict_ = corpora.Dictionary(clean_corpus)\n",
    "\n",
    "print(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f3be8",
   "metadata": {},
   "source": [
    "### The dictionary had 52 unqiue words in the cleaned corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c08fe0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n",
      "want\n",
      "watch\n",
      "weekend\n",
      "beating\n",
      "championship\n",
      "eight\n",
      "india\n",
      "new\n",
      "shopping\n",
      "southampton\n",
      "test\n",
      "went\n",
      "wicket\n",
      "world\n",
      "yesterday\n",
      "zealand\n",
      "amazon\n",
      "cricket\n",
      "don’t\n",
      "good\n",
      "netflix\n",
      "prime\n",
      "book\n",
      "chill\n",
      "however\n",
      "it’s\n",
      "like\n",
      "long\n",
      "nice\n",
      "paint\n",
      "read\n",
      "time\n",
      "way\n",
      "would\n",
      "blueberry\n",
      "dispenza’s\n",
      "dr\n",
      "gamechanger\n",
      "helped\n",
      "joe\n",
      "learn\n",
      "milkshake\n",
      "much\n",
      "reading\n",
      "try\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "for i in dict_.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9001ea",
   "metadata": {},
   "source": [
    "### Converting list of documents (corpus) into Document Term Matrix using the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0fbbfd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(0, 1), (2, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
       " [(0, 1),\n",
       "  (20, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1)],\n",
       " [(20, 1),\n",
       "  (23, 2),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9107de",
   "metadata": {},
   "source": [
    "### Creating the object for LDA model using gensim library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "492a518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc4e8b",
   "metadata": {},
   "source": [
    "### Running and Training LDA model on the document term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e5dcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0, eval_every=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a0924",
   "metadata": {},
   "source": [
    "### Prints the topics with the indexes: 0,1,2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a700e908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.100*\"watch\" + 0.054*\"movie\" + 0.054*\"good\" + 0.054*\"amazon\" + 0.054*\"netflix\" + 0.054*\"cricket\" + 0.054*\"prime\" + 0.054*\"don’t\" + 0.028*\"shopping\" + 0.027*\"world\"'),\n",
       " (1,\n",
       "  '0.054*\"went\" + 0.052*\"yesterday\" + 0.052*\"beating\" + 0.052*\"wicket\" + 0.049*\"india\" + 0.049*\"new\" + 0.049*\"eight\" + 0.049*\"championship\" + 0.049*\"test\" + 0.048*\"southampton\"'),\n",
       " (2,\n",
       "  '0.053*\"movie\" + 0.053*\"good\" + 0.053*\"paint\" + 0.053*\"book\" + 0.053*\"it’s\" + 0.053*\"way\" + 0.053*\"like\" + 0.053*\"chill\" + 0.053*\"nice\" + 0.053*\"would\"'),\n",
       " (3,\n",
       "  '0.021*\"movie\" + 0.021*\"watch\" + 0.021*\"good\" + 0.021*\"weekend\" + 0.021*\"want\" + 0.021*\"book\" + 0.021*\"cricket\" + 0.021*\"prime\" + 0.021*\"don’t\" + 0.021*\"gamechanger\"'),\n",
       " (4,\n",
       "  '0.021*\"watch\" + 0.021*\"movie\" + 0.021*\"good\" + 0.021*\"want\" + 0.021*\"book\" + 0.021*\"don’t\" + 0.021*\"weekend\" + 0.021*\"prime\" + 0.021*\"gamechanger\" + 0.021*\"cricket\"'),\n",
       " (5,\n",
       "  '0.081*\"book\" + 0.043*\"try\" + 0.043*\"good\" + 0.043*\"helped\" + 0.043*\"reading\" + 0.043*\"dispenza’s\" + 0.043*\"dr\" + 0.043*\"much\" + 0.043*\"joe\" + 0.043*\"blueberry\"')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics()\n",
    "\n",
    "# we need to manually check whethere the topics are different from one another or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca17c2",
   "metadata": {},
   "source": [
    "### Extracting topics from the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6dde00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.100*\"watch\" + 0.054*\"movie\" + 0.054*\"good\" + 0.054*\"amazon\" + 0.054*\"netflix\"'), (1, '0.054*\"went\" + 0.052*\"yesterday\" + 0.052*\"beating\" + 0.052*\"wicket\" + 0.049*\"india\"'), (2, '0.053*\"movie\" + 0.053*\"good\" + 0.053*\"paint\" + 0.053*\"book\" + 0.053*\"it’s\"'), (3, '0.021*\"movie\" + 0.021*\"watch\" + 0.021*\"good\" + 0.021*\"weekend\" + 0.021*\"want\"'), (4, '0.021*\"watch\" + 0.021*\"movie\" + 0.021*\"good\" + 0.021*\"want\" + 0.021*\"book\"'), (5, '0.081*\"book\" + 0.043*\"try\" + 0.043*\"good\" + 0.043*\"helped\" + 0.043*\"reading\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=6, num_words=5))\n",
    "\n",
    "# num_topics mean: how many topics want to extract \n",
    "# num_words: the number of words that want per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29725ed",
   "metadata": {},
   "source": [
    "### Printing the topic associations with the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d8b3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc :  0 [(0, 0.033789564), (1, 0.03333515), (2, 0.03345683), (3, 0.033337004), (4, 0.033337004), (5, 0.8327445)]\n",
      "doc :  1 [(0, 0.011953344), (1, 0.9404239), (2, 0.011905277), (3, 0.011906146), (4, 0.011906146), (5, 0.01190519)]\n",
      "doc :  2 [(0, 0.91657203), (1, 0.016667273), (2, 0.016708594), (3, 0.016667888), (4, 0.016667888), (5, 0.016716292)]\n",
      "doc :  3 [(0, 0.011129042), (1, 0.0111115705), (2, 0.9443921), (3, 0.011112034), (4, 0.011112034), (5, 0.011143234)]\n",
      "doc :  4 [(0, 0.010426467), (1, 0.010417157), (2, 0.010434489), (3, 0.0104176495), (4, 0.0104176495), (5, 0.9478866)]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in ldamodel[doc_term_matrix]:\n",
    "    print(\"doc : \",count,i)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57dd785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
